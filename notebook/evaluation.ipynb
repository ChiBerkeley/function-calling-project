{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant trained to assist\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the process of backpropagation in nerual networks.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nAccording to the information I have, the weather in USA is currently Sunday.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '59360e82-a97b-4fb2-938f-44fd800d1164', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 13 Oct 2024 00:19:55 GMT', 'content-type': 'application/json', 'content-length': '261', 'connection': 'keep-alive', 'x-amzn-requestid': '59360e82-a97b-4fb2-938f-44fd800d1164'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 372}}, id='run-007158fd-1f1d-464b-b3d4-60348420d2ec-0', usage_metadata={'input_tokens': 55, 'output_tokens': 17, 'total_tokens': 72})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model = \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    temperature = 0.4,\n",
    "    max_tokens=None\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a bot and you should reply to the user based on the function calling return. The return is {'weather': 'Sunday', 'location': 'USA'}\"),\n",
    "    (\"user\", \"what is the weather in USA now?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"[{\n",
    "  \"query\": \"How many times does the letter 'a' appear in the word 'banana'?\",\n",
    "  \"answer\": {\n",
    "    \"function\": \"count_letter\",\n",
    "    \"arguments\": {\n",
    "      \"word\": \"banana\",\n",
    "      \"letter\": \"a\"\n",
    "    },\n",
    "    \"result\": {\n",
    "      \"result\": \"3\"\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"query\": \"How many times does the letter 'e' appear in the word 'computer'?\",\n",
    "  \"answer\": {\n",
    "    \"function\": \"count_letter\",\n",
    "    \"arguments\": {\n",
    "      \"word\": \"computer\",\n",
    "      \"letter\": \"e\"\n",
    "    },\n",
    "    \"result\": {\n",
    "      \"result\": \"2\"\n",
    "    }\n",
    "  }\n",
    "}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.kevin_function import count_letter\n",
    "\n",
    "## read test file\n",
    "with open (\"ks_instruct/test.txt\", \"r\") as file:\n",
    "    text = file.read() # it cannot be read currently\n",
    "\n",
    "example = json.loads(example_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def call_function(info: dict):\n",
    "    query = info[\"query\"]\n",
    "    answer = info[\"answer\"]\n",
    "    func_to_call = eval(answer[\"function\"])\n",
    "    arguments = answer[\"arguments\"]\n",
    "    return {\"query\": query, \"output\": func_to_call(**arguments)}\n",
    "\n",
    "def generate_response(query_output, llm):\n",
    "    messages = [\n",
    "    (\"system\", f\"You are a bot and you should reply to the user based on the function calling return. The return is {query_output[\"output\"]}\"),\n",
    "    (\"user\", query_output[\"query\"])]\n",
    "    response = llm.invoke(messages)\n",
    "    query_output[\"response\"] = response.content\n",
    "    return query_output\n",
    "\n",
    "query_output = call_function(example[0])\n",
    "query_response = generate_response(query_output, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"How many times does the letter 'a' appear in the word 'banana'?\",\n",
       " 'answer': {'function': 'count_letter',\n",
       "  'arguments': {'word': 'banana', 'letter': 'a'},\n",
       "  'result': {'result': '3'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query_response\n",
    "example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the actual output directly answers the question about the frequency of the letter 'a' in the word 'banana', making it highly relevant and accurate.\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class AWSBedrock(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"llama-3-1-8b\"\n",
    "\n",
    "aws_bedrock = AWSBedrock(model=llm)\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.3,\n",
    "    model=aws_bedrock,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=query_response[\"query\"],\n",
    "    actual_output= query_response[\"response\"]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "# evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': [\"How many times does the letter 'a' appear in the word 'banana'?\",\n",
       "  \"What is the frequency of the letter 'a' in the word 'banana'?\",\n",
       "  \"How often does the letter 'a' appear in the word 'banana'?\"]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse prompt engineering\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"reverse prompt engineering output\"\"\"\n",
    "    query: List[str] = Field(description=\"list of queries according to the user repsonse\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Query)\n",
    "\n",
    "def get_reverse_prompt(n, query_response, llm):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Guess \" + str(n) + \"most possible queries based on a user's response.\\n{format_instructions}\\n{query}\\n\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    query_response[\"response\"]\n",
    "    prompt_and_model = prompt | llm\n",
    "    output = prompt_and_model.invoke({\"query\": query_response[\"response\"]})\n",
    "\n",
    "    return parser.invoke(output)\n",
    "\n",
    "def calculate_mean_similiarity(query_response, reverse_queries):\n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "    # def get_cos_sim(query_response, reverse_queries):\n",
    "    embedding = BedrockEmbeddings(model_id=model_id)\n",
    "    original_embedded = embedding.embed_query(query_response[\"query\"])\n",
    "    guess_embeded = embedding.embed_documents(reverse_queries[\"query\"])\n",
    "    return np.array([cosine_similarity([original_embedded], [item])[0] for item in guess_embeded]).mean()\n",
    "\n",
    "reverse_queries = get_reverse_prompt(3, query_response, llm)\n",
    "reverse_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"How many times does the letter 'a' appear in the word 'banana'?\",\n",
       " 'output': '{\"result\": 3}',\n",
       " 'response': \"\\n\\nThe letter 'a' appears 3 times in the word 'banana'.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/cs/Library/Caches/pypoetry/virtualenvs/function-calling-project-_d8EX7TE-py3.12/lib/python3.12/site-packages\n",
       "/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "The score is 0.50 because the actual output incorrectly claims the function count_letter is to be called, and also incorrectly identifies the first and second parameters as 'pear' and 'b' respectively, which are not mentioned in the retrieval context.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3</span><span style=\"color: #374151; text-decoration-color: #374151\">-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama-\u001b[0m\u001b[1;38;2;55;65;81m3\u001b[0m\u001b[38;2;55;65;81m-\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m-8b, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:04,  4.06s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Faithfulness (score: 0.6666666666666666, threshold: 0.7, strict: False, evaluation model: llama-3-1-8b, reason: The score is 0.67 because the actual output contains errors: the function to be called is incorrect and the first parameter is also incorrect, indicating a moderate level of faithfulness to the retrieval context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Which function and argument I should call if I ask: How many times does the letter 'a' appear in the word 'banana'?\n",
      "  - actual output: We should call a function named count_letter('pear', 'b'))\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['\\n    Counts the number of occurrences of a specified letter in a word.\\n\\n    Parameters:\\n    word (str): The word to count letters in.\\n    letter (str): The letter to count.\\n\\n    Example:\\n    >>> count_letter(\\'strawberry\\', \\'R\\')\\n    \\'{\"result\": 3}\\'\\n    ']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Faithfulness: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=False, metrics_data=[MetricData(name='Faithfulness', threshold=0.7, success=False, score=0.6666666666666666, reason='The score is 0.67 because the actual output contains errors: the function to be called is incorrect and the first parameter is also incorrect, indicating a moderate level of faithfulness to the retrieval context.', strict_mode=False, evaluation_model='llama-3-1-8b', error=None, evaluation_cost=None, verbose_logs='Truths (limit=None):\\n[\\n    \"A function to count the number of occurrences of a specified letter in a word exists.\",\\n    \"The function takes two parameters: \\'word\\' and \\'letter\\'.\",\\n    \"The function returns a JSON object.\"\\n] \\n \\nClaims:\\n[\\n    \"We should call a function named count_letter(\\'pear\\', \\'b\\'))\",\\n    \"A function named count_letter exists\",\\n    \"The function count_letter takes two parameters\",\\n    \"The first parameter of the function count_letter is \\'pear\\'\",\\n    \"The second parameter of the function count_letter is \\'b\\'\",\\n    \"The function count_letter is to be called\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The actual output claims the function count_letter is to be called, which is not correct as the retrieval context states the function count_letter exists instead.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The actual output claims the first parameter of the function count_letter is \\'pear\\', which is not correct as the retrieval context does not mention the first parameter.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input=\"Which function and argument I should call if I ask: How many times does the letter 'a' appear in the word 'banana'?\", actual_output=\"We should call a function named count_letter('pear', 'b'))\", expected_output=None, context=None, retrieval_context=['\\n    Counts the number of occurrences of a specified letter in a word.\\n\\n    Parameters:\\n    word (str): The word to count letters in.\\n    letter (str): The letter to count.\\n\\n    Example:\\n    >>> count_letter(\\'strawberry\\', \\'R\\')\\n    \\'{\"result\": 3}\\'\\n    '])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# {'query': \"How many times does the letter 'a' appear in the word 'banana'?\",\n",
    "#  'answer': {'function': 'count_letter',\n",
    "#   'arguments': {'word': 'banana', 'letter': 'a'},\n",
    "#   'result': {'result': '3'}}}\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We should call a function named count_letter('pear', 'b'))\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [count_letter.__doc__]\n",
    "\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=aws_bedrock,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Which function and argument I should call if I ask: How many times does the letter 'a' appear in the word 'banana'?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Counts the number of occurrences of a specified letter in a word.\\n\\n    Parameters:\\n    word (str): The word to count letters in.\\n    letter (str): The letter to count.\\n\\n    Example:\\n    >>> count_letter(\\'strawberry\\', \\'R\\')\\n    \\'{\"result\": 3}\\'\\n    '"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_letter.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "function-calling-project-_d8EX7TE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
